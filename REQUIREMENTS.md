# HAMMER (Hands-on Ansible Multi-node Machine Evaluation Runner)

## v1 requirements specification

Deterministic assignment authoring, generation, and auto-grading for Ansible
(Vagrant + libvirt + AlmaLinux 9, multi-node, fully autogenerated tests, local execution)

### 1. Scope and objectives

The system shall generate complete Ansible lab assignments from a single declarative specification. For each assignment, the system shall produce:

1. A student bundle:

* Multi-node Vagrant environment using libvirt/KVM and AlmaLinux 9
* Student starter repository structure (playbooks, roles, defaults, inventory)
* Fully autogenerated pytest tests that students can run locally
* Documentation on how to run the lab and tests

2. A grading bundle:

* A separate inventory root with group_vars and host_vars used for grading-time overrides
* A grading ansible.cfg selected via ANSIBLE_CONFIG
* Vault secrets and vault invocation configuration
* Fully autogenerated pytest tests (core tests are identical to student tests)
* Optional additional hidden edge-case tests (also autogenerated, not hand-written)
* A grading runner that executes defined phases and exports artifacts

3. A lock artifact that guarantees determinism and rebuildability:

* Spec hash
* Seed
* Resolved mutation values
* Pinned versions
* Checksums of generated artifacts

No handwritten pytest test files are permitted in v1. All tests must be generated from the spec.

The system will run locally on the instructor machine (you). No CI integration is required in v1.

---

### 2. Non-goals and explicit limits

* The system will not generate the correct Ansible solution automatically.
* The grader will not enforce exact module selection, task ordering, or role structure beyond minimal entrypoints and file presence required for execution.
* The system will not attempt to prove variable origin (for example, that a value came from group_vars rather than vars_files). Instead, it enforces variable-driven solutions through controlled overrides, mutation phases, and observable bindings.

---

### 3. Runtime environment requirements

#### 3.1 Virtualization

* Provider: libvirt/KVM via Vagrant.
* Vagrant environment must be multi-node.
* Vagrant runs must be headless and non-interactive.

#### 3.2 Target OS

* AlmaLinux 9 only in v1.
* Base box must be pinned to a specific version per assignment lock.

#### 3.3 Controller requirements (instructor machine)

* ansible-core pinned per lock.
* Python dependencies pinned per lock:

  * pytest
  * testinfra
  * ansible-runner
  * any helper libs used by the grading runner

---

### 4. High-level architecture

Input:

* Assignment specification file (YAML) that defines topology, variable contracts, precedence scenarios, behavioral contracts, mutation phases, and vault contracts.

Generation pipeline:

* Validate spec against a strict schema.
* Resolve all seeded values deterministically.
* Emit student bundle and grading bundle.
* Emit generated pytest tests.
* Emit a grading runner that orchestrates Vagrant, Ansible runs, and pytest.

Execution pipeline:

* Bring up Vagrant VMs.
* Run Ansible in multiple phases with controlled variable overlays.
* Collect ansible-runner events and stats.
* Run generated pytest tests per node (sequential per node).
* Produce score and artifacts.

---

### 5. Assignment specification (DSL)

The spec is the single source of truth. It must be expressive enough that tests can be generated mechanically.

#### 5.1 Metadata

Required:

* assignment_id (string)
* version (string)
* seed (int)
* provider: libvirt
* os: almalinux9

#### 5.2 Topology

Defines nodes and their group membership (used for inventory generation and group_vars logic).

Required:

* nodes: list of:

  * name (string, stable)
  * groups (list of strings)
  * resources:

    * cpu (int)
    * ram_mb (int)
* networks:

  * private networks, optionally forwarded ports, if the assignment needs them
* optional: inter-node dependencies (for test ordering or service reachability checks)

#### 5.3 Entry points and student deliverables

Defines what the runner will execute.

Required:

* playbook_path (default: site.yml at repo root)
* optional role expectations:

  * required role names (for scaffolding only)
  * required variable names (enforced by behavior, not file placement)
* inventory expectations:

  * students must be able to run using provided student inventory

#### 5.4 Variable contracts

This section is required because all tests are autogenerated and must avoid hardcoded values.

Each variable contract includes:

* name: variable name, for example http_port
* type: int|string|bool|list|dict
* student_default: value used in the student bundle to enable local development
* grading_overlay:

  * overlay_source: grading inventory root
  * overlay_kind: group_vars|host_vars|inventory_vars
  * overlay_target:

    * group name (if group_vars)
    * host name (if host_vars)
* mutation_values:

  * explicit list of values used across phases (at least two for anti-hardcoding)
* bindings:

  * list of observable binding targets where the variable must appear or take effect, for example:

    * service_listen_port: { service: nginx, protocol: tcp, address: 0.0.0.0 }
    * firewall_port_open: { zone: public, protocol: tcp }
    * template_contains: { path: /etc/nginx/nginx.conf, pattern: "listen {{ value }}" }
    * file_contains: { path: /etc/app/config, pattern: "PORT={{ value }}" }

Rules:

* Any variable that is a learning objective must have at least one binding target.
* Any binding target must be testable on the target OS without relying on unstable output parsing.

#### 5.5 Precedence scenarios

Defines layered variable values across precedence sources and the expected effective value.

Each scenario includes:

* name
* variable
* layers (any subset):

  * role_default
  * role_vars
  * play_vars
  * vars_files (and include_vars)
  * inventory_vars
  * group_vars
  * host_vars
  * extra_vars
* expected_layer (which layer should win)
* bindings_to_verify: reference to bindings defined in variable contracts

Rules:

* v1 must support generating at least:

  * group_vars overrides role defaults
  * host_vars overrides group_vars
  * extra_vars overrides all
  * inventory vars vs group_vars (at least one scenario)

Note: the grader enforces precedence by controlling the overlays and then verifying the binding outcomes, not by checking student repository structure.

#### 5.6 Behavioral contracts

Defines required system behaviors not strictly tied to a single variable.

Supported contract families (v1):

* packages:

  * name, present/absent, node_group
* services:

  * name, enabled, running, node_group
* firewall:

  * open ports, zones, protocols, derived from a variable or constant
* selinux:

  * file contexts, booleans, if needed
* users/groups:

  * presence, membership, home, shell
* files:

  * existence, permissions, owner/group, content regex
* network reachability:

  * service on node A reachable from node B

#### 5.7 Handler and conditional contracts

This is required because you want complex conditionals and handlers graded robustly.

Each handler contract includes:

* name (logical label)
* node_group (or host)
* handler_target:

  * service name and action type: restart|reload
* trigger_conditions:

  * list of triggers that should cause the handler to run, for example:

    * file_changed: /etc/nginx/nginx.conf
    * template_changed: /etc/app/app.conf
    * variable_changed: http_port
* non_trigger_conditions:

  * list of situations where handler must not run, for example:

    * noop_rerun: true
    * unrelated_file_changed: /etc/some/other.conf
* verification_modes: both must be enabled in v1:

  1. system-observable verification (Testinfra):

     * check service restart timestamp changed only when expected
  2. runner-event verification (ansible-runner):

     * verify notify and handler execution events occurred only when expected

Rules:

* Handler behavior must be checked in at least two phases (baseline vs mutation) plus a no-op rerun.
* The grader must not rely solely on runner events, and must not rely solely on service timestamps. Both are required.

#### 5.8 Idempotence policy

Required:

* idempotence.required: true|false (default true)
* idempotence.allowed_changes:

  * optional allowlist of known acceptable changes (rare, should be empty by default)
* idempotence.enforcement:

  * require changed == 0 on the idempotence phase
  * require no handlers executed on idempotence phase unless explicitly allowed

Idempotence must be validated using:

* ansible-runner stats and per-task changed flags
* plus at least one system-level invariant test (for example config file hash unchanged) for critical files

#### 5.9 Vault contracts

v1 must support vault-based assignments.

Spec must define:

* vault_ids (optional, supported)
* vaulted_vars_files to be created in student bundle as encrypted placeholders
* which variables are sourced from vault
* binding targets for vaulted values (file content, service config, etc.)

Grading bundle must include:

* vault password file(s) or vault-id mapping files
* runner must execute ansible-playbook / ansible-runner non-interactively with vault access

---

### 6. Generated artifacts

#### 6.1 Student bundle

Must include:

* Vagrantfile (libvirt multi-node)
* student inventory
* group_vars and host_vars with student_default values
* scaffolding playbook/role structure (minimal, not overconstraining)
* autogenerated pytest test suite:

  * tests refer to resolved variables at runtime, not constants
  * tests runnable locally without secret grading overlays
* README with:

  * vagrant up
  * ansible-playbook invocation
  * pytest invocation

#### 6.2 Grading bundle

Must include:

* grading inventory root directory:

  * inventory files
  * group_vars and host_vars overlays for precedence and mutation phases
* grading ansible.cfg used via ANSIBLE_CONFIG
* vault password provisioning
* autogenerated pytest tests:

  * the core tests must be identical to student tests
  * additional hidden tests may exist, also autogenerated
* grading runner entrypoint:

  * CLI or Python module, whichever is simplest
  * takes student repo path and produces score and artifacts

#### 6.3 Lock artifact

Must include:

* spec hash
* seed
* resolved mutation values per phase
* pinned versions:

  * AlmaLinux box version
  * ansible-core
  * pytest/testinfra/ansible-runner
* checksums of generated outputs

---

### 7. Grading runner behavior

#### 7.1 Phase model (normative)

Minimum phases required in v1:

Phase 1: baseline converge

* Apply baseline overlays from grading inventory root (group_vars/host_vars/inventory vars)
* Run playbook on all nodes

Phase 2: baseline verify

* Run pytest tests per node, sequentially per node
* Export junit xml

Phase 3: mutation converge

* Apply mutation overlays (variable values changed, and optional precondition mutations)
* Run playbook on all nodes

Phase 4: mutation verify

* Run pytest tests per node, sequentially per node
* Additionally verify handler expectations via ansible-runner events for this phase

Phase 5: idempotence converge

* Re-run playbook with the same inputs as mutation converge

Phase 6: idempotence verify

* Require changed == 0 (unless allowed_changes)
* Require no handlers executed (unless explicitly allowed)
* Run pytest tests per node, sequentially per node

#### 7.2 Per-node execution policy

* Tests must run per node, sequentially, to simplify failure attribution and avoid cross-node timing issues.
* Multi-node contracts (reachability) must be generated as tests that run on a defined node and assert connectivity to others.

#### 7.3 Data capture requirements

Runner must capture:

* vagrant logs
* ssh config extraction results
* ansible-runner artifacts directory for each phase:

  * events
  * status
  * stats
* playbook stdout/stderr
* pytest output and junit xml
* a summarized scoring JSON

---

### 8. Autogenerated tests (requirements)

The generator must emit pytest tests that use two complementary verification channels.

#### 8.1 Testinfra-based system verification

For each contract and binding, generate tests that assert real system state:

* service enabled/running
* socket listening on port derived from resolved variables
* firewall rules effective (prefer verifying listening + firewall command output checks when stable)
* SELinux context correct for target files
* template/file content contains expected rendered values

Key requirement:

* Any expected value that is variable-driven must be derived from resolved runtime vars, not hardcoded.

#### 8.2 ansible-runner event-based verification

Generate tests or runner-side checks that inspect runner event streams to validate:

* which tasks changed
* which handlers were notified
* which handlers executed
* idempotence (changed count)
* conditional task execution paths when the spec defines them as requirements

Event-based checks must be tied to spec contracts, for example:

* handler contract expects exactly one handler execution in mutation converge and zero in idempotence converge.

#### 8.3 Anti-hardcoding enforcement

For every variable contract with bindings:

* The generator must create at least one mutation phase where the variable value changes.
* The tests must re-verify all bindings after mutation.
* This makes copying a value into a template insufficient because the second value differs.

---

### 9. Determinism rules

* Any randomized choice must come from the seed and be stored in the lock.
* Mutation values are explicitly specified in the spec in v1 (no generator randomness required).
* All external dependencies must be pinned:

  * Vagrant box version
  * Python packages
  * ansible-core

---

### 10. Implementation minimalism (v1)

To keep v1 achievable:

* Prefer a small Python CLI with two subcommands:

  * build: generate bundles + lock
  * grade: run phases on a student repo path and output artifacts
* Keep contract types limited to the list in sections 5.6 and 5.7.
* Defer advanced features like parallel grading, web UI, plagiarism detection, or auto-difficulty scaling.

---

## What I will need next (only if you want this to be very concrete)

To turn this into an implementable schema, I would normally ask for:

* The exact node naming convention you want (web1, web2, db1, etc.)
* Whether you want to standardize on a specific service stack (nginx/httpd, mariadb/postgresql) for built-in contract templates
* Whether SELinux enforcement should be strict by default or opt-in per contract

If you do not answer these, a v1 can still proceed with defaults:

* node names are taken as provided
* service names are literal strings in the spec
* SELinux checks only run when the spec includes selinux contracts

